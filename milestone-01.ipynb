{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "742adba1-54cd-47b6-b445-d82cb2250c64",
   "metadata": {},
   "source": [
    "# Template Notebook for Milestones\n",
    "\n",
    "In this notebook you will write your code, producing the required output for each Milestone.\n",
    "\n",
    "Your notebook must contain 3 types of cells:\n",
    "\n",
    "- (1) Code cells: Cells that contain code snippets, capturing one cohesive fragment of your code.\n",
    "- (2) Corresponding explanation cells: Each code cell must be followed by a text cell containing the **english** explanation of what the corresponding code cell does and what it's purpose is\n",
    "- (3) One reflection cell: One cell at the bottom of the notebook that contains your individual reflection on your process working on this milestones in **english**. It could contain technical problems and how you overcame them, it could contain social problems and how you deal with them (group work is hard!), it could contain explanations of prior skills or knowledge that made certain parts of the task easier for you, etc... (those are just suggestions. Your individual reflections will of course contain different/additional aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25fa832d-c0ed-41dd-b4a9-68580c1ae974",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12/122612973.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     ]\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"authors_str\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'authors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    744\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0mdata_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m             )\n\u001b[1;32m   1142\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def global_init(): #initiate pyterrier\n",
    "    if not pt.started():\n",
    "        pt.init()\n",
    "def merge_columns(dataframe, new_column_name, delimiter, columns): #create extra column text which cosists of all relevant fields merged together to have all important tokens in one place\n",
    "    dataframe[new_column_name] = dataframe[columns].apply(lambda x: delimiter.join(x.values), axis=1)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "global_init() \n",
    "ex_path = os.getcwd() #path creation\n",
    "src_path = f'{ex_path}\\\\src'\n",
    "# index_path = f'{src_path}\\\\index'\n",
    "dataset_path = f'{src_path}\\\\ir-anthology-07-11-2021-ss23.jsonl'\n",
    "dataset_path_processed = f'{src_path}\\\\ir-anthology-07-11-2021-ss23-processed.jsonl'\n",
    "\n",
    "columns = [ #relevant columns\n",
    "        \"id\", \n",
    "        \"abstract\", \n",
    "        \"editors\", \n",
    "        \"authors\", \n",
    "        \"year\", \n",
    "        \"booktitle\",\n",
    "        \"publisher\"\n",
    "    ]\n",
    "\n",
    "dataframe = pd.read_json(dataset_path, lines=True)[columns] #pandas to read out. from here\n",
    "\n",
    "dataframe[\"authors_str\"] = [', '.join(map(str, l)) for l in dataframe['authors']]\n",
    "dataframe = dataframe.fillna(\"\").astype(str)\n",
    "\n",
    "# dataframe = dataframe.iloc[[2]]\n",
    "#to here\n",
    "dataframe = merge_columns(dataframe, \"text\", \" \", [\"booktitle\", \"abstract\", \"authors_str\", \"publisher\"])\n",
    "#merged dataframe\n",
    "dataframe = dataframe.rename(columns={\"id\" : \"doc_id\"})\n",
    "\n",
    "# print(dataframe.columns)\n",
    "\n",
    "file = open(dataset_path_processed, \"w+\")\n",
    "file.write(dataframe.to_json(lines=True, orient='records'))#safe results\n",
    "file.close()\n",
    "\n",
    "print(f'File created at: {dataset_path_processed}')#the end. you will find garbagecode that we used to try out a few things. not all of it. just snippets\n",
    "\n",
    "# create index from the dataframe\n",
    "# indexer = pt.DFIndexer(None, type=pt.index.IndexingType.MEMORY) \n",
    "# indexer = pt.DFIndexer(index_path, overwrite=True, type=pt.index.IndexingType.CLASSIC)\n",
    "    \n",
    "# print(dataframe.loc[:, ~dataframe.columns.isin(['title'])].to_json(indent=2))\n",
    "# input abstract as text for the body and the dataframe with all columns excludet text and title. title because we already have booktitle.\n",
    "# index_ref = indexer.index(dataframe[\"text\"], dataframe.loc[:, ~dataframe.columns.isin(['text', 'title'])])\n",
    "    \n",
    "# load the index, print the statistics\n",
    "# index = pt.IndexFactory.of(index_ref)\n",
    "# print(index.getCollectionStatistics().toString())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Example Code cell.\n",
    "\n",
    "# Here are a few of many snippets to show that we have experimented already.\n",
    "\n",
    "\n",
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "import os\n",
    "#from transformers import AutoTokenizer\n",
    "#from collections import Counter\n",
    "\n",
    "import json\n",
    "cwd = os.getcwd()\n",
    "path = f'{cwd}/ir-anthology-07-11-2021-ss23.jsonl'\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "    print(\"pr started...\")\n",
    "\n",
    "    \n",
    "def merge_columns(df,id_col):\n",
    "    cols_to_merge = [col for col in df.columns if col !=id_col]\n",
    "    \n",
    "    #merged = pd.DataFrame({'text':df[cols].apply(lambda x: ' '.join(x.values.astype(str)),axis=1)})\n",
    "    #return pd.concat([df[id_col],merged],axis=1)\n",
    "    \n",
    "\n",
    "anthology = pd.read_json(path,lines=True)\n",
    "\n",
    "#anthology=anthology.rename(columns={\"booktitle\":\"text\"})\n",
    "anthology=anthology.rename(columns={\"id\":\"docno\"})\n",
    "\n",
    "column_list = [\"abstract\", \"booktitle\", \"publisher\", \"year\", \"authors\"]\n",
    "#b= merge_columns(anthology,column_list,anthology[\"docno\"])\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "anthology_indexer = pt.DFIndexer(\"./pd_index\")\n",
    "indexref_anthology = anthology_indexer.index(anthology[\"text\"],anthology[\"docno\"])\n",
    "\n",
    "if not os.path.exists(\"./pd_index\"):\n",
    "    indexref_anthology = anthology_indexer.index(anthology[\"text\"],anthology[\"docno\"])\n",
    "    print(\"#########\")\n",
    "anthology_index = pt.IndexFactory.of(indexref_anthology)\n",
    "\n",
    "print(anthology_index.getCollectionStatistics().toString())   \n",
    "\n",
    "\n",
    "#print(\"\\n\\n\",pt.BatchRetrieve(anthology_index,wmodel=\"TF_IDF\")) #just a test\n",
    "    \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "\"\"\"\n",
    "\"\"\" Fallen Code-snippets. Shall they find peace.\n",
    "{\"text\":anthology[\"text\"],\"docno\":anthology[\"docno\"]}\n",
    "\n",
    "def read_dataset(line):\n",
    "    import json\n",
    "    data = json.loads(line)\n",
    "    return data[\"id\"],data[\"abstract\"]\n",
    "\n",
    "\n",
    "indexer = pt.DFIndexer(\"./index\")\n",
    "indexer.index(path,meta=(\"docno\",\"text\"),meta_dn=read_dataset)\n",
    "#print(anthology[\"text\"])\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "anthology2 = anthology[\"text\"].str.cat(anthology[\"booktitle\"],sep=\" \")\n",
    "\n",
    "#bm25 = pt.BatchRetrieve(anthology_indexer,wmodel=\"PL2\")\n",
    "#anthology.to_csv(\"andthologyDF.csv\")\n",
    "\n",
    "#anthology.rename(columns={\"id\":\"stud_id\"})\n",
    "#topics kreieren:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "file = open(\"ir-anthology-07-11-2021-ss23.jsonl\",\"r\")\n",
    "#print(file.readline())\n",
    "rows=file.readlines()\n",
    "file.close()\n",
    "tmp_list=[]\n",
    "for line in rows[1:]:\n",
    "    tmp_list.append([json.loads(line)[\"id\"],json.loads(line)[\"abstract\"]])\n",
    "\n",
    "\n",
    "df = pd.read_json(path,lines=True)\n",
    "pd_indexer = pt.DFIndexer(\"./pd_index\")\n",
    "indexref = pd_indexer.index(df[\"abstract\"],df[\"id\"])\n",
    "\n",
    "\n",
    "#pt.init()\n",
    "#iter_indexer = pt.IterDictIndexer(\"./index\",meta={\"doc_id\":\"id\",\"text\":\"abstract\"})\n",
    "#indexref1 = iter_indexer.index(rows)\n",
    "print('This is an example code cell. Write your first code snippet here.')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0440f20e-3c64-44c3-8537-f5065f792f8a",
   "metadata": {},
   "source": [
    "### Example Explanation cell\n",
    "\n",
    "The above cell prints a sentence in order to give an impression of what might be done with such a cell. This cell here explains it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ab4a36-0f0b-46d2-99a4-56e792f70bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more code cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bcc247-ee10-4db4-ac59-876377b5ce9c",
   "metadata": {},
   "source": [
    "### Add More Explanation cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f614e3-7fe6-4d84-9f25-47fc9b341ccb",
   "metadata": {},
   "source": [
    "### Example Reflection Cell\n",
    "\n",
    "Working on this notebook was difficult, because it is not the real notebook but just an example. My experience in writing notebooks helped speed up the process, for example knowing that there are different types of cells. However, it was difficult to figure our what to write exactly in order to make sure the students understand what they are supposed to do as I could not test it before giving it to the students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d9c7e9-8228-4f4f-b942-d460a35eee31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTo be honest, we spent more time figuring out what to do than actually being able to work\\non the material.\\nFirst we thought we should learn pyterrier. We spent 3 days figuring out what the commands do\\nand then we finally understood that we are not doing what we were supposed to do.\\nBut at least we created several indexes in several ways. \\n\\nEventually we had to modify our code, we used pandas as a tool in  python to create an\\nextra field named \"text\" where we decided to merge all relevant columns that we thought would\\nbe useful for tokenisation later.\\n\\nThen we learned how to use docker properly and created our own dockerfiles but it was pretty\\ntimeconsuming and more trial&error than we have would liked it to be.\\n\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "To be honest, we spent more time figuring out what to do than actually being able to work\n",
    "on the material.\n",
    "First we thought we should learn pyterrier. We spent 3 days figuring out what the commands do\n",
    "and then we finally understood that we are not doing what we were supposed to do.\n",
    "But at least we created several indexes in several ways. \n",
    "\n",
    "Eventually we had to modify our code, we used pandas as a tool in  python to create an\n",
    "extra field named \"text\" where we decided to merge all relevant columns that we thought would\n",
    "be useful for tokenisation later.\n",
    "\n",
    "Then we learned how to use docker properly and created our own dockerfiles but it was pretty\n",
    "timeconsuming and more trial&error than we have would liked it to be.\n",
    "\n",
    "after you published the example we straight threw our attemts away \n",
    "and just modifyed your Dockerfile. Sorry for that.\n",
    "\n",
    "But we took some good lessons. We dived a litte bit into tira and how it works, we discussed\n",
    "a lot. we spent time together working through the books and the material and we definetily\n",
    "know which way this course is going.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9fc17a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ec334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
